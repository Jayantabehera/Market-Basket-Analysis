{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Libraries\n",
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "#import pyfpgrowth\n",
    "from efficient_apriori import apriori\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark1 = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark1.read.csv(\"D:\\Data Mining and Machine Learning 2\\Project\\BigTempRb.txt\", sep=',',header='true', inferSchema='true',escape=\"\\\"\" )\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select columns and check row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"RequestTimestamp\",\"ResponseRgBasketId\",\"RequestSiteId\",\n",
    "      \"RequestBasketValue\",\"RequestBasketId\",\n",
    "      \"RequestNumberBasketItems\",\"RequestBasketJsonString\")\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ItemList column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UDF to extract item list from json\n",
    "def get_item_list(item_json):\n",
    "    #print(item_json)\n",
    "    response = json.loads(item_json)\n",
    "    lister = []\n",
    "    for nest in response['items']:\n",
    "        lister.append(nest[\"b\"])\n",
    "    return (list(set(lister)))\n",
    "\n",
    "### converting above function to a UDF\n",
    "item_list = udf(get_item_list, ArrayType(StringType()))\n",
    "\n",
    "## Creating item_list from JsonString\n",
    "df = df.withColumn('ItemList', item_list(df.RequestBasketJsonString))\n",
    "#df.show()\n",
    "\n",
    "#load ItemList to transactions list\n",
    "transactions = df.select(\"ItemList\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input paramters\n",
    "input_support = .0002 ## VALUE RANGES BETWEEN 0 AND 1\n",
    "input_confidence = 0.001 ## VALUE RANGES BETWEEN 0 AND 1\n",
    "############### Applying Pyspark FPgrowth algorithm ######################\n",
    "#fpGrowth = FPGrowth(itemsCol=\"ItemList\", minSupport=input_support, minConfidence=input_confidence)\n",
    "#model = fpGrowth.fit(df)\n",
    "itemsets, rules = apriori(transactions, min_support=input_support, min_confidence=input_confidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print rules for validation purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out every rule with 1 items on the left hand side,\n",
    "# 1 item on the right hand side, sorted by lift\n",
    "rules_rhs = filter(lambda rule: len(rule.lhs) == 1 and len(rule.rhs) == 1, rules)\n",
    "for rule in sorted(rules_rhs, key=lambda rule: rule.lift):\n",
    "  print(rule)  # Prints the rule and its confidence, support, lift, ...\n",
    "    \n",
    "\n",
    "\n",
    "# Print out every rule with 2 items on the left hand side,\n",
    "# 1 item on the right hand side, sorted by lift\n",
    "rules_rhs = filter(lambda rule: len(rule.lhs) == 2 and len(rule.rhs) == 1, rules)\n",
    "for rule in sorted(rules_rhs, key=lambda rule: rule.lift): \n",
    "  print(rule)  # Prints the rule and its confidence, support, lift, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
